# Agent Analytics Dashboard

View agent usage statistics and performance metrics

**Usage:** `/agent-metrics [period]`

**Input:** $ARGUMENTS (optional: 7d, 30d, 90d, all)

---

## Instructions

Display comprehensive analytics about agent usage, success rates, and performance patterns.

### 1. Determine Time Period

Parse $ARGUMENTS for time period:
- `7d` or `week` â†’ Last 7 days
- `30d` or `month` â†’ Last 30 days (default)
- `90d` or `quarter` â†’ Last 90 days
- `all` or `lifetime` â†’ All time

Default to 30 days if not specified.

### 2. Run Analytics Script

```bash
# Run the enhanced analytics script
bash ~/.claude/scripts/agent-analytics-enhanced.sh "$PERIOD" 2>/dev/null

# If not available, fall back to basic script
if [ $? -ne 0 ]; then
  bash ~/.claude/scripts/agent-analytics.sh "$PERIOD" 2>/dev/null
fi

# If both unavailable, perform manual analysis
if [ $? -ne 0 ]; then
  echo "âš ï¸  Analytics scripts unavailable, performing manual analysis"
fi
```

### 3. Parse PROJECT_CONTEXT.md

If scripts unavailable, manually parse:

```bash
# Find all PROJECT_CONTEXT.md files
find . -name "PROJECT_CONTEXT.md" 2>/dev/null

# Extract agent activity entries
grep -E "^\*\*.*\*\* - \`.*\`" PROJECT_CONTEXT.md 2>/dev/null

# Count by agent name
# Track success/failure markers
# Calculate time periods
```

### 4. Generate Analytics Report

Present comprehensive analytics:

```
ğŸ“ˆ AGENT ANALYTICS DASHBOARD
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Period: [Last 30 days / Last 7 days / etc.]
Data Source: [PROJECT_CONTEXT.md + logs]
Last Updated: [timestamp]

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š USAGE STATISTICS

Top 10 Most Used Agents:
   1. backend-architect        23 runs  (92% success)  â­â­â­â­â­
   2. frontend-developer       18 runs  (89% success)  â­â­â­â­â­
   3. test-engineer            15 runs  (100% success) â­â­â­â­â­
   4. code-reviewer            12 runs  (95% success)  â­â­â­â­â­
   5. debugger                 10 runs  (80% success)  â­â­â­â­
   6. api-designer              8 runs  (88% success)  â­â­â­â­
   7. security-practice-reviewer 7 runs (100% success) â­â­â­â­â­
   8. database-optimizer        6 runs  (83% success)  â­â­â­â­
   9. code-architect            5 runs  (100% success) â­â­â­â­â­
  10. refactoring-specialist    4 runs  (75% success)  â­â­â­â­

Total Agent Invocations: 108
Unique Agents Used: 10 of 43 available

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ¯ SUCCESS RATES

Highest Success Rate (â‰¥5 runs):
   â€¢ test-engineer              100% (15/15)
   â€¢ code-architect             100% (5/5)
   â€¢ security-practice-reviewer 100% (7/7)
   â€¢ code-reviewer               95% (12/12)

Needs Improvement (<80% success):
   â€¢ refactoring-specialist      75% (3/4)
   â€¢ debugger                    80% (8/10)

   ğŸ’¡ Low success may indicate:
      - Insufficient context provided
      - Complex tasks beyond agent scope
      - Tool/dependency issues

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”— AGENT CHAINS

Most Common Agent Sequences:

1. api-designer â†’ backend-architect â†’ test-engineer
   Used: 8 times
   Success: 100%
   Use Case: API feature development

2. code-architect â†’ database-optimizer â†’ backend-architect
   Used: 5 times
   Success: 80%
   Use Case: Full-stack features with DB

3. frontend-developer â†’ test-engineer
   Used: 6 times
   Success: 100%
   Use Case: UI component development

4. debugger â†’ test-engineer
   Used: 4 times
   Success: 75%
   Use Case: Bug fixing with test coverage

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â±ï¸  PERFORMANCE METRICS

Average Execution Time by Agent:
   â€¢ frontend-developer         ~8 min
   â€¢ backend-architect         ~12 min
   â€¢ test-engineer             ~10 min
   â€¢ code-reviewer              ~5 min
   â€¢ debugger                  ~15 min

Fastest Agents (Efficiency):
   1. code-reviewer             ~5 min
   2. api-designer              ~6 min
   3. frontend-developer        ~8 min

Slowest Agents (Complexity):
   1. debugger                 ~15 min (complex investigation)
   2. backend-architect        ~12 min (implementation)
   3. database-optimizer       ~11 min (schema design)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“… USAGE TRENDS

Weekly Activity:
   Week 1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 45 invocations
   Week 2: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 35 invocations
   Week 3: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 38 invocations
   Week 4: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 30 invocations

Peak Activity Days:
   â€¢ Monday:    28 invocations
   â€¢ Tuesday:   24 invocations
   â€¢ Wednesday: 22 invocations

Agent Category Distribution:
   Full-Stack:     35% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
   Testing:        22% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
   Code Quality:   18% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
   Performance:    12% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
   Security:       10% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
   Other:           3% â–ˆâ–ˆ

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸš¨ FAILURE ANALYSIS

Total Failures: 12 (11% of invocations)

Top Failure Reasons:
   1. Missing dependencies        5 failures
   2. Insufficient context        3 failures
   3. Tool unavailable            2 failures
   4. Validation loop failure     2 failures

Agents with Most Failures:
   â€¢ debugger                     3 failures
   â€¢ refactoring-specialist       2 failures
   â€¢ deployment-engineer          2 failures

ğŸ’¡ Recommendations:
   â€¢ Provide more context for debugger tasks
   â€¢ Check tool availability before refactoring
   â€¢ Improve deployment environment setup

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“ LEARNING INSIGHTS

Underutilized Agents (0-2 uses):
   â€¢ ai-engineer (0 uses)
   â€¢ blockchain-developer (0 uses)
   â€¢ mobile-app-developer (1 use)
   â€¢ game-developer (0 uses)
   â€¢ trading-bot-strategist (0 uses)

   ğŸ’¡ Consider if these agents could help with:
      - Adding AI features
      - Mobile app development
      - Specialized domains

Optimal Agent Combinations:
   âœ… api-designer before backend-architect (100% success)
   âœ… test-engineer after any implementation agent (98% success)
   âœ… code-reviewer as final step (100% success)
   âš ï¸  debugger alone has lower success (80%)
      â†’ Better with test-engineer follow-up

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ’° COST EFFICIENCY (Tokens)

Average Token Usage by Agent:
   â€¢ backend-architect     ~45K tokens
   â€¢ frontend-developer    ~38K tokens
   â€¢ test-engineer         ~32K tokens
   â€¢ code-reviewer         ~18K tokens

Total Token Usage: ~4.2M tokens this period
Average per Invocation: ~39K tokens

Most Efficient Agents (tokens/success):
   1. code-reviewer        ~18K tokens
   2. api-designer         ~22K tokens
   3. test-engineer        ~32K tokens

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“‹ RECOMMENDATIONS

Based on your usage patterns:

âœ… Keep Doing:
   â€¢ Using test-engineer after implementations (100% success)
   â€¢ api-designer â†’ backend-architect chain works great
   â€¢ code-reviewer as quality gate is effective

ğŸ”§ Improvements:
   â€¢ Add more context to debugger tasks (80% â†’ target 90%+)
   â€¢ Consider refactoring-specialist earlier in dev cycle
   â€¢ Pre-check dependencies for deployment-engineer

ğŸ’¡ Explore:
   â€¢ Try performance-profiler for optimization tasks
   â€¢ Use security-practice-reviewer proactively
   â€¢ Consider ai-engineer for intelligent features

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ” DETAILED BREAKDOWN

View detailed agent info:
   /list-agents [category]

Get agent suggestions for task:
   /agent-select [task description]

View project status:
   /status
```

---

## Data Sources

Metrics collected from:
1. **PROJECT_CONTEXT.md** - Agent activity logs
2. **Agent logs** (if available) - Execution details
3. **Git history** - Commit correlation
4. **Validation results** - Success/failure tracking

---

## Metric Definitions

**Success Rate:**
- Percentage of agent invocations that completed without errors
- Validation loops passed
- No retry required

**Execution Time:**
- Time from agent launch to completion
- Includes all retries and validation

**Agent Chain:**
- Sequence of agents used in order
- Extracted from PROJECT_CONTEXT.md workflow

**Failure:**
- Agent reported error/blocker
- Validation loops failed
- Required manual intervention

---

## Examples

**Default (last 30 days):**
```
/agent-metrics
/agent-metrics 30d
```

**Weekly report:**
```
/agent-metrics 7d
/agent-metrics week
```

**Quarterly analysis:**
```
/agent-metrics 90d
/agent-metrics quarter
```

**All-time stats:**
```
/agent-metrics all
/agent-metrics lifetime
```

---

## Export Options

**Export to CSV:**
```bash
# Generate CSV report
bash ~/.claude/scripts/agent-analytics-enhanced.sh 30d --format=csv > agent-metrics.csv
```

**Export to JSON:**
```bash
# Generate JSON report
bash ~/.claude/scripts/agent-analytics-enhanced.sh 30d --format=json > agent-metrics.json
```

---

## Notes

- Metrics help optimize agent usage and task quality
- Success rates indicate context completeness
- Agent chains reveal common workflows
- Failure analysis guides improvement areas
- Use insights to improve future task descriptions
- Underutilized agents may indicate untapped capabilities
